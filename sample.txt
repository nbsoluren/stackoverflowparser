{"question": {"description": "What is the quickest way to HTTP GET in Python if I know the content will be a string? I am searching the docs for a quick one-liner like:\n\tcontents = url.get(\"http://example.com/foo/bar\")\n\nBut all I can find using Google are httplib and urllib - and I am unable to find a shortcut in those libraries.\n\thttplib\n\turllib\nDoes standard Python 2.5 have a shortcut in some form as above, or should I write a function url_get?\n\turl_get\n\twget\n\tcurl\n", "authors": ["nbro", "Frank Krueger"], "upvotes": "", "comments": [{"score": "\n23\n", "comment": "One-liners are not necessarily faster.  Don't fetishize code golf.  You have to measure speed; not lines of code.", "author": "S.Lott"}, {"score": "\n74\n", "comment": "uhm, no, I googled in here because I needed to add a line to an experiment I'm writing; not the finished product. CPU time is much, much cheaper than programmer time!", "author": "Phlip"}, {"score": "\n", "comment": "I found what I needed here: stackoverflow.com/a/385411/1695680", "author": "ThorSummoner"}]}, "answers": [{"description": "Python 2.x:\n\timport urllib2\ncontents = urllib2.urlopen(\"http://example.com/foo/bar\").read()\n\nPython 3.x:\n\timport urllib.request\ncontents = urllib.request.urlopen(\"http://example.com/foo/bar\").read()\n\nDocumentation for urllib.request and read.\nHow is that?\n", "authors": ["ctrl-alt-delor", "Nick Presta"], "upvotes": "", "comments": [{"score": "\n27\n", "comment": "Does everything get cleaned up nicely? It looks like I should call close after your read. Is that necessary?", "author": "Frank Krueger"}, {"score": "\n4\n", "comment": "It is good practice to close it, but if you're looking for a quick one-liner, you could omit it. :-)", "author": "Nick Presta"}, {"score": "\n16\n", "comment": "The object returned by urlopen will be deleted (and finalized, which closes it) when it falls out of scope. Because Cpython is reference-counted, you can rely on that happening immediately after the read. But a with block would be clearer and safer for Jython, etc.", "author": "sah"}, {"score": "\n4\n", "comment": "It doesn't work with HTTPS-only websites. requests works fine", "author": "OverCoder"}, {"score": "\n6\n", "comment": "If you're using Amazon Lambda and need to get a URL, the 2.x solution is available and built-in. It does seem to work with https as well.  It's nothing more than r = urllib2.urlopen(\"http://blah.com/blah\") and then text = r.read(). It is sync, it just waits for the result in \"text\".", "author": "Fattie"}]}, {"description": "You could use a library called requests.\n\timport requests\nr = requests.get(\"http://example.com/foo/bar\")\n\nThis is quite easy. Then you can do like this:\n\t>>> print r.status_code\n>>> print r.headers\n>>> print r.content\n\n", "authors": [], "upvotes": "", "comments": [{"score": "\n2\n", "comment": "I notice this is not available in Amazon Lambda...", "author": "Fattie"}, {"score": "\n", "comment": "@JoeBlow remember that you must import the  external libraries in order to use them", "author": "MikeVelazco"}, {"score": "\n", "comment": "Almost any Python library can be used in AWS Lambda. For pure Python, you just need to \"vendor\" that library (copy into your module's folders rather than using pip install). For non-pure libraries, there's an extra step -- you need to pip install the lib onto an instance of AWS Linux (the same OS variant lambdas run under), then copy those files instead so you'll have binary compatibility with AWS Linux. The only libraries you won't always be able to use in Lambda are those with binary distributions only, which are thankfully pretty rare.", "author": "Chris Johnson"}, {"score": "\n", "comment": "this doesn't work with python3.", "author": "lawphotog"}, {"score": "\n1\n", "comment": "@lawphotog this DOES work with python3, but you have to pip install requests.", "author": "akarilimano"}]}, {"description": "If you want solution with httplib2 to be oneliner consider instatntinating anonymous Http object\n\timport httplib2\nresp, content = httplib2.Http().request(\"http://example.com/foo/bar\")\n\n", "authors": ["to-chomik"], "upvotes": "", "comments": []}, {"description": "Have a look at httplib2, which - next to a lot of very useful features - provides exactly what you want.\n\timport httplib2\n\nresp, content = httplib2.Http().request(\"http://example.com/foo/bar\")\n\nWhere content would be the response body (as a string), and resp would contain the status and response headers.\nIt doesn't come included with a standard python install though (but it only requires standard python), but it's definitely worth checking out.\n", "authors": ["hennr"], "upvotes": "", "comments": []}, {"description": "theller's solution for wget is really useful, however, i found it does not print out the progress throughout the downloading process. It's perfect if you add one line after the print statement in reporthook.\n\timport sys, urllib\n\ndef reporthook(a, b, c):\n    print \"% 3.1f%% of %d bytes\\r\" % (min(100, float(a * b) / c * 100), c),\n    sys.stdout.flush()\nfor url in sys.argv[1:]:\n    i = url.rfind(\"/\")\n    file = url[i+1:]\n    print url, \"->\", file\n    urllib.urlretrieve(url, file, reporthook)\nprint\n\n", "authors": ["Xuan"], "upvotes": "", "comments": []}, {"description": "Here is a wget script in Python:\n\t# From python cookbook, 2nd edition, page 487\nimport sys, urllib\n\ndef reporthook(a, b, c):\n    print \"% 3.1f%% of %d bytes\\r\" % (min(100, float(a * b) / c * 100), c),\nfor url in sys.argv[1:]:\n    i = url.rfind(\"/\")\n    file = url[i+1:]\n    print url, \"->\", file\n    urllib.urlretrieve(url, file, reporthook)\nprint\n\n", "authors": ["theller"], "upvotes": "", "comments": []}, {"description": "Excellent solutions Xuan, Theller.\nFor it to work with python 3 make the following changes\n\timport sys, urllib.request\n\ndef reporthook(a, b, c):\n    print (\"% 3.1f%% of %d bytes\\r\" % (min(100, float(a * b) / c * 100), c))\n    sys.stdout.flush()\nfor url in sys.argv[1:]:\n    i = url.rfind(\"/\")\n    file = url[i+1:]\n    print (url, \"->\", file)\n    urllib.request.urlretrieve(url, file, reporthook)\nprint\n\nAlso, the URL you enter should be preceded by a \"http://\", otherwise it returns a unknown url type error.\n", "authors": ["Akshar"], "upvotes": "", "comments": []}, {"description": "If you are working with HTTP APIs specifically, there are also more convenient choices such as Nap.\nFor example, here's how to get gists from Github since May 1st 2014:\n\tfrom nap.url import Url\napi = Url('https://api.github.com')\n\ngists = api.join('gists')\nresponse = gists.get(params={'since': '2014-05-01T00:00:00Z'})\nprint(response.json())\n\nMore examples: https://github.com/kimmobrunfeldt/nap#examples\n", "authors": ["Kimmo"], "upvotes": "", "comments": []}, {"description": "Without further necessary imports this solution works (for me) - also with https:\n\ttry:\n    import urllib2 as urlreq # Python 2.x\nexcept:\n    import urllib.request as urlreq # Python 3.x\nreq = urlreq.Request(\"http://example.com/foo/bar\")\nreq.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36')\nurlreq.urlopen(req).read()\n\nI often have difficulty grabbing the content when not specifying a \"User-Agent\" in the header information. Then usually the requests are cancelled with something like: urllib2.HTTPError: HTTP Error 403: Forbidden or urllib.error.HTTPError: HTTP Error 403: Forbidden.\n\turllib2.HTTPError: HTTP Error 403: Forbidden\n\turllib.error.HTTPError: HTTP Error 403: Forbidden\n", "authors": ["michael_s"], "upvotes": "", "comments": []}]}